# CSC4008 Assignment 1

CHEN Ang (118010009) $\newcommand{\vect}[1]{\boldsymbol{#1}} \newcommand{\v}[1]{\boldsymbol{#1}}$$\newcommand{\ba}{\begin{aligned}}\newcommand{\ea}{\end{aligned}}$ $\newcommand{\t}{^*}$ $\newcommand{\rint}{\int_{-\infty}^{\infty}}$ $\newcommand{\inv}{^{-1}}$

## 1.2

We have that $\vect X = [X_1, X_2, X_3]^∗\sim \mathcal N(\v 0, \v\Sigma)$. Denote
$$
\v\Sigma\equiv\left[\begin{array}{cc}
\v\Sigma_{12} & \v\Sigma_{12,3} \\
\v\Sigma_{3,12} & \Sigma_{3}
\end{array}\right]
$$
where $\v\Sigma_{12}\in \R^{2\times2}$, $\v\Sigma_{12,3}=\v\Sigma_{3,12}\t\in\R^2$, and $\Sigma_3\in \R$.

By block matrix inversion, $\newcommand{\a}{\v\Sigma_{12}} \newcommand{\b}{\v\Sigma_{12,3}} \newcommand{\c}{\v\Sigma_{12,3}\t} \newcommand{\d}{\Sigma_{3}}$
$$
\v\Theta=
\left[\begin{array}{cc}
\v \Theta_{12} & \v \Theta_{12,3} \\
\v \Theta_{3,12} & \Theta_3
\end{array}\right]
$$
where
$$
\ba

\v \Theta_{12}   &= (\a-\b\d\inv\c)\inv\\
\Theta_{3}       &= (\d-\c\a\inv\b)\inv =  \d\inv+\d\inv\c\v\Theta_{12}\b\d\inv\\
\v \Theta_{12,3} &= \v\Theta_{3,12}\t   =  -\v\Theta_{12}\b\d\inv

\ea\tag1
$$
**Lemma 0.** The marginal distribution of $X_3$ is $\mathcal N(0,\Sigma_3)$.

*Proof.* Consider the joint PDF
$$
f_{\v X}(\v x)=\frac{1}{(2\pi)^{\frac 32}|\v\Sigma|^{\frac12}}e^{-\frac12 \v x\t
\v\Theta\v x}=\frac{1}{(2\pi)^{\frac 32}|\v\Sigma|^{\frac12}}e^{-\frac12 Q(\v x)}
$$
where we define
$$
\ba

Q(\v x)&\equiv\v x\t
\v\Theta\v x\\

&=\v x_{12}\t\v\Theta_{12}\v x_{12}+2\v x_{12}\t \v\Theta_{12,3}x_3+\Theta_3x_3^2\\

&=		\v x_{12}\t       \v\Theta_{12}   \v x_{12}\\
&\quad-2\v x_{12}\t \v\Theta_{12}\b\d\inv x_3\\
&\quad+	   			\left(\d\inv+\d\inv\c\v\Theta_{12}\b\d\inv\right)x_3^2\\

&=	 \d\inv x_3^2\\
&\quad+
\left(
\v x_{12}\t\v\Theta_{12}\v x_{12}
-2\v x_{12}\t \v\Theta_{12}\b\d\inv x_3
+\d\inv\c\v\Theta_{12}\b\d\inv x_3^2
\right)\\

&=\d\inv x_3^2\\
&\quad+
\left(
\v x_{12}-\b\d\inv x_3
\right)\t \v\Theta_{12}\left(
\v x_{12}-\b\d\inv x_3
\right)
\ea
$$
Define
$$
\ba
\v m&\equiv \v m(x_3)=\b\d\inv x_3\\
Q_{12}(\v x)&\equiv(\v x_{12}-\v m)\t\v\Theta_{12}(\v x_{12}-\v m)\\
Q_3(x_3)&\equiv \d\inv x_3^2
\ea
$$
we have
$$
\ba
Q(\v x)&=(\v x_{12}-\v m)\t\v\Theta_{12}(\v x_{12}-\v m)+\d\inv x_3^2\\
&=Q_{12}(\v x)+Q_3(x_3)
\ea
$$
Thus the joint PDF can rewritten as
$$
\ba

f_{\v X}(\v x)&=\frac{1}{(2\pi)^{\frac 32}|\v\Sigma|^{\frac12}}e^{-\frac12 Q_{12}(\v x)}e^{-\frac12 Q_{3}(x_3)}


\ea
$$
Computing the determinant block-wise, one has
$$
|\v\Sigma|=|\a-\b\d\inv\c||\d|=|\v \Theta_{12}\inv|\d
$$
Hence
$$
\ba
f_{\v X}(\v x)&=
\frac{1}{(2\pi)^{\frac 22}|\v \Theta_{12}\inv|^{\frac12}}e^{-\frac12 Q_{12}(\v x)}
\cdot\frac{1}{(2\pi)^{\frac 12}\d^{\frac12}}e^{-\frac12 Q_{3}(x_3)}\\

&=\mathcal N\left(\v x_{12};\ \v m,\v\Theta_{12}\inv\right)\cdot
\mathcal N\left(\v x_{3};\ 0,\d\right)

\ea\tag2
$$
Therefore marginal distribution of $X_3$ has density
$$
\ba
f_{X_3}(x_3)&=\rint\rint f_{\v X}(\v x)\ dx_1dx_2\\
&=
\mathcal N\left(\v x_{3};\ 0,\d\right)\rint\rint \mathcal N\left(\v x_{12};\ \v m,\v\Theta_{12}\inv\right)\ dx_1dx_2\\
&=
\mathcal N\left(\v x_{3};\ 0,\d\right)
\ea
$$
which completes the proof.

**Lemma 1.** The conditional distribution of $[X_1,X_2]\t$ given $X_3=x_3$ is normal with mean vector
$$
\v\mu_{12|3}= \b\d\inv x_3
$$
and covariance matrix
$$
\ba
\v\Sigma_{12|3}&=\v\Sigma_{12}-\Sigma_3^{-1}\v\Sigma_{12,3}\v\Sigma_{12,3}\t\\
&=
\left[\begin{array}{cc}
\Sigma_{1}-\Sigma_{1,3}^2\Sigma_{3}^{-1} & \Sigma_{1,2}-\Sigma_{1,3}\Sigma_{2,3}\Sigma_{3}^{-1} \\
\Sigma_{2,1}-\Sigma_{2,3}\Sigma_{1,3}\Sigma_{3}^{-1} & \Sigma_{2}-\Sigma_{2,3}^2\Sigma_{3}^{-1}
\end{array}\right]


\ea
$$
*Proof.* Using Lemma 0 and equation $(2)$,
$$
\ba
f_{\v{X}_{12}|X_3=x_3}(\v x_{12})&=\frac{f_{\v X}(\v x)}{f_{X_3}(x_3)}\\
&=
\frac{\mathcal N\left(\v x_{12};\ \v m,\v\Theta_{12}\inv\right)\cdot
\mathcal N\left(\v x_{3};\ 0,\d\right)}{
\mathcal N\left(\v x_{3};\ 0,\d\right)}\\

&=\mathcal N\left(\v x_{12};\ \v m,\v\Theta_{12}\inv\right)

\ea
$$
where $\v m=\b\d\inv x_3=\v\mu_{12|3}$ and $\v\Theta_{12}\inv=\a-\b\d\inv\c=\v\Sigma_{12|3}$, the desired result.

**Lemma 2.** The conditional distribution of $X_1$ given $X_3=x_3$ is normal with mean
$$
\mu_{1|3}=(\v\mu_{12|3})_1={x_3}\Sigma_{1,3}{\Sigma_3}^{-1}
$$
and variance
$$
\sigma^2_{1|3}=(\v\Sigma_{12|3})_{1,1}=\Sigma_{1}-\Sigma_{1,3}^2\Sigma_{3}^{-1}
$$
and similarly for $X_2$ given $X_3=x_3$.

*Proof.* This result directly follows from Lemma 0 and 1.

----

*Proof of the main theorem.* For clarity, denote
$$
\left[\begin{array}{cc}
a & b\\ b & c
\end{array}\right]\equiv\left[\begin{array}{cc}
\Sigma_{1}-\Sigma_{1,3}^2\Sigma_{3}^{-1} & \Sigma_{1,2}-\Sigma_{1,3}\Sigma_{2,3}\Sigma_{3}^{-1} \\
\Sigma_{2,1}-\Sigma_{2,3}\Sigma_{1,3}\Sigma_{3}^{-1} & \Sigma_{2}-\Sigma_{2,3}^2\Sigma_{3}^{-1}
\end{array}\right]=\v\Sigma_{12|3}
$$
so that
$$
\sigma^2_{1|3}=a,\ \sigma^2_{2|3}=c
$$
$“\implies”:$

Suppose $X_1,X_2$ are conditionally independent given $X_3$. Then
$$
\ba
f_{X_1,X_2|X_3=x_3}(x_1,x_2)&=f_{X_1|X_3=x_3}(x_1)\cdot f_{X_2|X_3=x_3}(x_2)\\
\ea
$$
Using Lemma 1 and 2, we substitute the PDFs to get
$$
\frac{1}{(2\pi)^{\frac 22}|\v\Sigma_{12|3}|^{\frac12}}e^{-\frac12 [x_1,x_2]
\v\Sigma_{12|3}^{-1}[x_1,x_2]\t}=\frac{1}{(2\pi)^{\frac {1+1}2}\sigma_{1|3}\sigma_{2|3}}e^{-\frac12\left({x_1^2}{/\sigma_{1|3}^2}+ {x_2^2}{/\sigma_{2|3}^2}\right)}
$$
Equating coefficients of $x_1,x_2$ in the exponents, the matrix
$$
\ba
\v\Sigma_{12|3}^{-1}&=
\left[\begin{array}{cc}
a^{-1}-b^2(a^2\Delta)^{-1} & b(a\Delta)^{-1} \\
b(a\Delta)^{-1} & -\Delta^{-1}
\end{array}\right]

\ea,\quad\Delta=b^2/a-c
$$
must be diagonal, and so
$$
b=\Sigma_{1,2}-\Sigma_{1,3}\Sigma_{2,3}\Sigma_{3}^{-1}=0\iff\Sigma_{1,2}\Sigma_{3}-\Sigma_{1,3}\Sigma_{2,3}=0
$$
Solving the inverse covariance matrix gives
$$
\theta_{1,2}=\frac{\Sigma_{1,2}\Sigma_{3}-\Sigma_{1,3}\Sigma_{2,3}}{\Sigma_1(\Sigma_{2,3}^2-\Sigma_{2}\Sigma_{3})+\Sigma_{1,2}^2\Sigma_{3}-2\Sigma_{1,2}\Sigma_{1,3}\Sigma_{2,3}+\Sigma_{1,3}^2\Sigma_{2}}=0
$$
which is to be shown.

$“\impliedby”:$

The other direction is similar. Suppose now $\theta_{1,2}=0.$ We then have
$$
\Sigma_{1,2}\Sigma_{3}-\Sigma_{1,3}\Sigma_{2,3}=0
$$
Since $\Sigma_3\neq 0$, we divide the equation through to get
$$
\Sigma_{1,2}-\Sigma_{1,3}\Sigma_{2,3}\Sigma_{3}^{-1}=b=0
$$
Therefore
$$
\v\Sigma_{12|3}=
\left[\begin{array}{cc}
a & 0\\ 0 & c
\end{array}\right],\ \v\Sigma_{12|3}^{-1}=\left[\begin{array}{cc}
a^{-1} & 0\\ 0 & c^{-1}
\end{array}\right]
$$
We evaluate the PDF of $[X_1,X_2]^*$ condition on $X_3$ using Lemma 1:
$$
\ba
f_{X_1,X_2|X_3=x_3}(x_1,x_2)&=\frac{1}{(2\pi)^{\frac 22}|\v\Sigma_{12|3}|^{\frac12}}e^{-\frac12 [x_1,x_2] \v\Sigma_{12|3}^{-1}[x_1,x_2]\t}\\
&=
\frac{1}{2\pi(ac)^{\frac12}}e^{-\frac12(x_1^2/a+x_2^2/c)}\\



\ea
$$
For the other side, we again use Lemma 2 to get
$$
\ba

f_{X_1|X_3=x_3}(x_1)\cdot f_{X_2|X_3=x_3}(x_2)&=

\frac{1}{(2\pi)^{\frac {1+1}2}\sigma_{1|3}\sigma_{2|3}}e^{-\frac12\left({x_1^2}{/\sigma_{1|3}^2}+ {x_2^2}{/\sigma_{2|3}^2}\right)}\\

&=

\frac{1}{2\pi a^{\frac12}c^{\frac12}}e^{-\frac12\left({x_1^2}{/a}+ {x_2^2}{/c}\right)}\\

\ea
$$
giving the desired equality, and we are done.

## 1.3

We have by definition
$$
\vect\Sigma\vect\Theta =
\left[\begin{array}{cc}
\boldsymbol{\Sigma}_{o} & \boldsymbol{\Sigma}_{o, h} \\
\boldsymbol{\Sigma}_{o, h}^{*} & \boldsymbol{\Sigma}_{h}
\end{array}\right]
\left[\begin{array}{cc}
\boldsymbol{\Theta}_{o} & \boldsymbol{\Theta}_{o, h} \\
\vect\Theta_{o, h}^{*} & \vect\Theta_{h}
\end{array}\right]=\v I
$$
Considering the first $n_o$ rows:
$$
\begin{aligned}

\boldsymbol{\Sigma}_{o}\boldsymbol{\Theta}_{o}+\boldsymbol{\Sigma}_{o, h}\boldsymbol{\Theta}_{o, h}^{*} &= \vect I\\
\boldsymbol{\Sigma}_{o}\boldsymbol{\Theta}_{o,h}+\boldsymbol{\Sigma}_{o, h}\boldsymbol{\Theta}_{h} &= \vect O

\end{aligned}
$$
The second equation gives us
$$
\vect{\Sigma}_{o,h}=-\boldsymbol{\Sigma}_{o}\boldsymbol{\Theta}_{o,h}\boldsymbol{\Theta}_{h}^{-1}
$$
Plugging back into the first equation,
$$
\boldsymbol{\Sigma}_{o}\left(\boldsymbol{\Theta}_{o}-\boldsymbol{\Theta}_{o, h}\boldsymbol{\Theta}_{h}^{-1}\right) = \vect I\iff \boldsymbol{\Sigma}_{o}^{-1} =\boldsymbol{\Theta}_{o}-\boldsymbol{\Theta}_{o, h}\boldsymbol{\Theta}_{h}^{-1}
$$

## 1.4

We have that
$$
\ba
e(\v x)&\equiv||\v y-\v{Ax}||_2^2\\
&=(\v y-\v{Ax})^*(\v y-\v{Ax})\\
&=(\v y^*-\v x^*\v A\t)(\v y-\v{Ax})\\
&=\v y\t \v y-\v y\t \v{Ax}-\v x\t\v A\t \v y+\v x\t\v A\t\v{Ax}\\
&=\v y\t \v y-2\v y\t \v{Ax}+\v x\t\v A\t\v{Ax}\\
\ea
$$
First Order Necessary Condition for local minima:
$$
\nabla e(\v x)=2(\v A\t\v{Ax}-\v A\t \v y)=\v 0\iff \v A\t\v{Ax}=\v A\t\v y
$$
Assuming that $\v A\t\v{A}$ is invertible (equivalently, $\text{rank}(\v A)=n$), FONC yields
$$
\v x_\star =(\v A\t\v{A})^{-1}\v A\t \v y
$$
Since the Hessian
$$
\nabla^2e(\v x)=2\v A\t \v A
$$
is positive semidefinite for all $\v x$, $e$ is convex. Further, because $\v x_\star$ is the unique critical point, it must also be the unique global minimum of $e$.

## 1.8

### 1

Consider
$$
\ba
e(\v x)&\equiv||\v y-\v{Ax}||_2^2+\lambda ||\v x||_2^2\\
&=(\v y-\v{Ax})^*(\v y-\v{Ax})+\lambda \v x\t\v x\\
&=\v y\t \v y-2\v y\t \v{Ax}+\v x\t\v A\t\v{Ax}+\lambda \v x\t\v x
\ea
$$
FONC:
$$
\nabla e(\v x)=2(\lambda \v x+\v A\t\v{Ax}-\v A\t \v y)=\v 0\iff (\v A\t\v{A}+\lambda\v I)\v{x}=\v A\t\v y
$$
Given that $\v A\t\v{A}+\lambda\v I$ is invertible, we obtain the critical point
$$
\boldsymbol{x}_{\star}=\left(\boldsymbol{A}^{*} \boldsymbol{A}+\lambda \boldsymbol{I}\right)^{-1} \boldsymbol{A}^{*} \boldsymbol{y}
$$
Since the Hessian
$$
\nabla^2e(\v x)=2(\v A\t \v A+\lambda \v I)
$$
is positive semidefinite for all $\v x$, $e$ is convex. Further, because $\v x_\star$ is the unique critical point, it must also be the unique global minimum of $e$.

### 2

Since $\v A^*\v A$ is real-symmetric, it is diagonalizable. Let $\lambda_1,\cdots \lambda_n$ be the eigenvalues of $\v A^*\v A$, with multiplicity. The matrix is invertible if and only if the determinant
$$
\ba

|\v A^*\v A+\lambda \v I|&= \prod_{i=1}^n(\lambda_i + \lambda)\neq0


\ea
$$
which holds precisely when
$$
\lambda_i\neq-\lambda,\quad\forall i
$$
If we are given $\lambda\gt 0$, then since $\v A^*\v A$ is positive semidefinite, we have
$$
\lambda_i\ge0\gt-\lambda,\quad\forall i
$$
The matrix $\v A\t \v A+\lambda \v I$ is always invertible.

