# CSC4008 Assignment 2

CHEN Ang (118010009) $\newcommand{\vect}[1]{\boldsymbol{#1}} \newcommand{\v}[1]{\boldsymbol{#1}}$$\newcommand{\ba}{\begin{aligned}}\newcommand{\ea}{\end{aligned}}$ $\newcommand{\t}{^\top}$ $\newcommand{\rint}{\int_{-\infty}^{\infty}}$ $\newcommand{\inv}{^{-1}}$

## 2.1

Recall from the lectures that if $p\ge 1$, $||\cdot||_p$ is a norm. Suppose given vectors $\v x,\v y$, one has for all $\lambda\in[0,1]$ that
$$
\ba
||\lambda \v x+(1-\lambda)\v y||_p&\le

||\lambda \v x||_p+||(1-\lambda)\v y||_p\quad\text{(Triangle Inequality)}
\\
&=\lambda ||\v x||_p +(1-\lambda)||\v y||_p\quad\text{(Scalability)}
\ea
$$
If $0\lt p\lt 1$, consider $\v x=[0;1],\v y=[1;0]$ for $\R^2$, and $\v x=[0;1;\v0 ]$ and $\v y=[1;0;\v 0]$ for higher dimensional spaces where $\v 0$ denotes zero vectors of suitable lengths. Then with $\lambda=1/2$,
$$
||\lambda \v x+(1-\lambda)\v y||_p=2^{1/p-1}\gt1=\lambda ||\v x||_p +(1-\lambda)||\v y||_p
$$

## 2.6

Denote by $s$ and $k$ the spark and krank of $\v A$, respectively. By definition of krank, $k$ is the maximum number of columns one can arbitrary choose from $\v A$ such that these columns are always linearly independent. Thus if one gets to choose one more column, namely $k+1$ columns, the requirement of krank would fail as there exists a set of $k+1$ columns from $\v A$ that are linear dependent. In other words, there exists $\v x\neq \v 0$ with $\v ||\v x||_0=k+1$ such that $\v A\v x=\v 0$. Hence we have $s\le k+1$. Further, because any $k$-subset of columns of $\v A$ is always linearly independent, there exists no $\v y\neq \v 0$ with $||\v y||_0=k$ such that $\v A \v y=\v 0$. Hence $s\gt k$, and we conclude that $s=k+1$.

## 2.10

Consider the SVD of matrix $\v A$ with full a row rank:
$$
\v A = \v U\v\Sigma\v V\t=\v U[\v\Sigma_1\ \ \v O]\left[\begin{array}{c}
\v{V}_{1}^{\top} \\
\v{V}_{2}^{\top}
\end{array}\right]=\v U\v \Sigma_1\v V_1\t
$$
The solution to $\v A\v x=\v y$ with the minimum 2-norm is given by
$$
\v x_m=\v A\t(\v A\v A\t)^{-1}\v y=\v V_1\v\Sigma_1^{-1}\v U\t\v y
$$


From last assignment, we know
$$
\nabla f(\v x)=2(\v A\t \v A\v x-\v A\t \v y)
$$
Writing $\beta:=2\alpha$, GD generates the sequence
$$
\ba
\v x_0&=\v 0\\
\v x_{k}&=\v B\v x_{k-1}+\v c\\
&=\v B^2\v x_{k-2}+\v B\v c + \v c\\
&\ \ \vdots\\
&=\v B^k\v x_0+\sum_{i=0}^{k-1}\v B^i \v c\\
&=\sum_{i=0}^{k-1}\v B^i \cdot \v c\\
\ea
$$
where $\v B:=\v I-\beta\v A\t \v A$ and $\v c:=\beta\v A\t \v y$. Notice that we cannot directly take $k\to\infty$ as
$$
\v I-\v B=\beta\v A\t \v A
$$
is singular. However, using SVD
$$
\ba
\v x_k&=\beta \sum_{i=0}^{k-1}\v (\v I-\beta\v V\v\Sigma\t\v\Sigma\v V\t)^i\cdot \v V\v\Sigma\t\v U\t \v y
\\&=\beta \sum_{i=0}^{k-1}\v (\v V\v V\t-\beta\v V\v\Sigma\t\v\Sigma\v V\t)^i\cdot \v V\v\Sigma\t\v U\t \v y\\
&=\beta \sum_{i=0}^{k-1}\v (\v V\v D\v V\t)^i\cdot \v V\v\Sigma\t\v U\t \v y\\
&= \beta \sum_{i=0}^{k-1}\v V\v D^i\v V\t \v V\v\Sigma\t\v U\t \v y\\
&=\beta \sum_{i=0}^{k-1}\v V\v D^i\v\Sigma\t\v U\t \v y
\ea
$$
where $\v D:=\v I-\beta\v\Sigma\t\v\Sigma $. Multiplying $\v V\t$ on both sides, we obtain
$$
\ba
\tilde{\v x}_k:=\v V\t\v x_k&=\beta\sum_{i=0}^{k-1}\v D^i\cdot\v\Sigma\t\v U\t \v y\\
&=
\beta\left[\begin{array}{c}
\sum_{i=0}^{k-1} (\v I-\beta\v\Sigma_1^2)^i& \v O\\
\v O & \v O
\end{array}
\right]

\v\Sigma\t\v U\t \v y

\ea
$$
If the step size is chosen such that for all singular value $\sigma$ of $\v A$,
$$
|1-\beta\sigma^2|\lt 1\iff\beta\in(0,2\sigma^{-2})\iff\alpha\in(0,\sigma^{-2})
$$
then as $k\to\infty$,
$$
\sum_{i=0}^{k-1} (\v I-\beta\v\Sigma_1^2)^i\to \beta^{-1}\v\Sigma_1^{-2}
$$
and so
$$
\tilde{\v x}_k\to\left[\begin{array}{c}
\v\Sigma_1^{-1}\\
\v O
\end{array}
\right]\v U\t \v y
$$
Multiplying by $\v V$ back,
$$
\ba
\v x_k\to \v x_\star&=\v V\left[\begin{array}{c}
\v\Sigma_1^{-1}\\
\v O
\end{array}
\right]\v U\t \v y\\&=\v V_1\v\Sigma_1^{-1}\v U\t\v y\\&=\v x_m

\ea
$$
and we're done.

## 2.13

### 1

$“\implies”$:

Suppose $\v z=\v {Bx}+\v{e}$ has a solution $(\v x,\v e)$ with $||\v e||_0=k$. Then by multiplying $\v A$ through in $(2.6.10)$,
$$
\v {Az}=\v{ABx}+\v{Ae}=\v A\v e
$$
$“\impliedby”$:

Suppose $\v {Ae}=\v{Az}$ has a solution $\v e$ with $||\v e||_0=k$. Then
$$
\v A(\v e-\v z)=\v 0\iff \v e-\v z\in\text{ker}(\v A)
$$
It's been given that the rows of $\v A$ span the left null space of $\v B$, i.e., $\text{im}(\v A\t)=\text{ker}(\v B\t)$. We also know by relationship of fundamental subspaces that $\ker(\v A)=\text{im}(\v A\t)^\perp$ and $\text{im}(\v B)=\text{ker}(\v B\t)^\perp$. Hence $\ker(\v A)=\text{im}(\v B)$, and so
$$
\v e-\v z\in \text{im}(\v B)\iff \v e-\v z=\v B\v x\iff \v z=\v B(-\v x)+\v e
$$
for some $\v x\in\R^r$.

### 2

Note that **1** also holds for L1 norm. (Simply replace $||\v e||_0$ by $||\v e||_1$ in the proof).


$\newcommand{\h}[1]{\hat{\v {#1}}}$Let $\hat{\v x}$ be a solution to $\min_\v x||\v {Bx}-\v z ||_{1}$. Then $\hat{\v e}=\v z-\v B\h x$ satisfies
$$
\v A\h e=\v {Az}-\v {AB}\h x=\v {Az}
$$
Moreover, $\hat{\v e}$ is optimal to $(2.6.13)$. To see that, assume there exists some $\v e\neq\h e$ such that $\v {Ae}=\v{Az}$ and $||\v e||_{1}\lt||\h e||_1$. By $“\impliedby”$, there must also exists $\v x'$ such that $\v z=\v B\v x'+\v e$. But then
$$
\min_\v x||\v B\v x-\v z||_1\le ||\v B\v x'-\v z||_1=||-\v e||_1=||\v e||_1\lt||\h e||_1=||\v B\h x-\v z||_1
$$
contradicting the optimality of $\h x$.

Now suppose $\h e$ is a solution to $(2.6.13)$. Due to the constraint it is also a solution to equation $(2.6.11)$. By $“\impliedby”$, there exists $\v x'$ such that $\v z=\v B\v x'+\h e$. We claim that $\v x'$ is a solution to $(2.6.12)$ with $\h e=\v z-\v B \v x'$. 

By way of contradiction assume there exists $\v x_*\neq \v x'$ such that $k:=||\v B\v x_*-\v z||_1\lt||\v B\v x'-\v z||_1$. Clearly, $(\v x_*,\v z-\v B\v x_*)$ is a solution to $(2.6.10)$ with $||\v z-\v B\v x_*||_1 =k$. Thus by $“\implies”$, $\v z-\v B\v x_*$ is also a solution to $(2.6.11)$ with $||\v z-\v B\v x_*||_1=k$. But then
$$
\min_{\v e}||\v e||_1\le ||\v z-\v B\v x_*||_1=k\lt||\v B\v x'-\v z||_1=||-\h e||_1=||\h e||_1
$$
contradicting the optimality of $\h e$.

